{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Login to fb @news page\n",
    "def login(driver, account, password): \n",
    "    \n",
    "    # Enter account\n",
    "    driver.find_element_by_xpath(\"//*[@id=\\\"m_login_email\\\"]\").send_keys(account)\n",
    "\n",
    "    # Enter password\n",
    "    driver.find_element_by_xpath(\"//*[@id=\\\"m_login_password\\\"]\").send_keys(password)\n",
    "\n",
    "    # Submit login\n",
    "    driver.find_element_by_xpath(\"//*[@id=\\\"u_0_5\\\"]\").click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Proceed to home page\n",
    "    driver.find_element_by_xpath('//*[@id=\"root\"]/div[1]/div/div/div[3]/div[1]/div/div/a').click()\n",
    "    time.sleep(2)\n",
    "\n",
    "# Get to article comment section\n",
    "def toCommentSection(page_id, driver, article_id):\n",
    "    # Generate url to each article comment section\n",
    "    url_article = 'https://mobile.facebook.com/story.php?story_fbid=' + article_id + '&id=' + page_id + '&fs=5&focus_composer=0&ref=page_internal'\n",
    "    driver.get(url_article)\n",
    "\n",
    "# Get link to actual article site @article page\n",
    "def getArticleLink(driver, article_index):\n",
    "    try: \n",
    "        article_link = driver.find_element_by_css_selector(\"a[class=\\\"touchable _4qxt\\\"]\").get_attribute(\"href\")\n",
    "        return article_link\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "# View more comments @article page\n",
    "def moreComments(driver):\n",
    "    try:\n",
    "        more_btn = driver.find_element_by_css_selector(\"a[class=\\\"_108_\\\"]\")\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        is_more = True\n",
    "    except:\n",
    "        is_more = False\n",
    "    while is_more:\n",
    "        try: \n",
    "            more_btn.click()\n",
    "            time.sleep(1.5)\n",
    "            current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if current_height == last_height:\n",
    "                is_more = False\n",
    "            else:\n",
    "                last_height = current_height\n",
    "        except:\n",
    "            is_more = False\n",
    "\n",
    "# Get comment and likes @each comment webelement\n",
    "def getCommentLikes(comment, article_index):\n",
    "    \n",
    "    # init return values\n",
    "    likes = '0'\n",
    "    has_sticker = 0\n",
    "    has_gif = 0\n",
    "    has_img = 0\n",
    "    has_vid = 0\n",
    "    num_friend_tags = 0\n",
    "    num_hashtags = 0\n",
    "    friend_text = []\n",
    "    hashtag_text = []\n",
    "    aria_text = \"\"\n",
    "    comment_text = \"\"\n",
    "    \n",
    "    # Get likes\n",
    "    try:\n",
    "        likes = comment.find_element_by_css_selector(\"span[class=\\\"_14va\\\"]\").text\n",
    "    except:\n",
    "        likes = '0'\n",
    "    \n",
    "    # Catch sticker\n",
    "    try:\n",
    "        comment.find_element_by_css_selector(\"i[class=\\\"img _2b1w img _2sxw\\\"]\")\n",
    "        has_sticker = 1\n",
    "    except:\n",
    "        # Catch GIF\n",
    "        try:\n",
    "            comment.find_element_by_css_selector(\"div[class=\\\"_14vb\\\"]\")\n",
    "            has_gif = 1 \n",
    "        except:\n",
    "            # Catch img\n",
    "            try:\n",
    "                img = comment.find_element_by_css_selector(\"div[class=\\\"_2b1t attachment\\\"]\")\n",
    "                has_img = 1\n",
    "                # Get img description\n",
    "                aria_text = img.find_element_by_css_selector(\"i[role=\\\"img\\\"]\").get_attribute(\"aria-label\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "                # Catch video\n",
    "                try:\n",
    "                    comment.find_element_by_css_selector(\"img[class=\\\"_34wo img\\\"]\")\n",
    "                    has_vid = 1\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "            # Get content\n",
    "            try:\n",
    "                body = comment.find_element_by_css_selector(\"div[data-sigil=\\\"comment-body\\\"]\")\n",
    "                \n",
    "                # Catch comments without text\n",
    "                try:\n",
    "                    body.find_element_by_css_selector(\"a[href*=\\\"fref=nf\\\"]\")\n",
    "                except:\n",
    "                    # Get comment text\n",
    "                    comment_text = body.text\n",
    "                    # Catch tags (friend_tags and hashtags)\n",
    "                    try:\n",
    "                        tags = body.find_elements_by_css_selector(\"a[href*=\\\"refid=52\\\"]\")\n",
    "                        friend_text = [friend.text for friend in tags]\n",
    "                        try:\n",
    "                            hashtags = body.find_elements_by_css_selector(\"a[href*=\\\"hashtag\\\"]\")\n",
    "                            hashtag_text = [hashtag.text for hashtag in hashtags]\n",
    "                            for text in hashtag_text:\n",
    "                                friend_text.remove(text)\n",
    "                            num_hashtags = len(hashtags)\n",
    "                            comment_text = replaceStr(comment_text, hashtag_text)\n",
    "                        except:\n",
    "                            pass\n",
    "                        num_friend_tags = len(tags) - num_hashtags\n",
    "                        if num_friend_tags != 0:\n",
    "                            comment_text = replaceStr(comment_text, friend_text)\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    # Change list to tuple to prevent numpy conversion ValueError\n",
    "    hashtag_text = ','.join(hashtag_text) \n",
    "    \n",
    "    # Return results as tuples\n",
    "    res_tuple = (article_index, comment_text,\n",
    "                 has_sticker, has_gif, has_img, has_vid, \n",
    "                 num_friend_tags, num_hashtags, hashtag_text, aria_text, likes)\n",
    "    return res_tuple\n",
    "\n",
    "def getStatusLikes(driver, article_index):\n",
    "    headline = ''\n",
    "    status = ''\n",
    "    has_headline = 0\n",
    "    has_vid = 0\n",
    "    has_img = 0\n",
    "    has_status = 0\n",
    "    likes = '0'\n",
    "    \n",
    "    try: \n",
    "        headline = driver.find_element_by_css_selector(\"span[data-ad-preview=\\\"headline\\\"]\").text\n",
    "        has_headline = 1\n",
    "    except:\n",
    "        headline = ''\n",
    "        try:\n",
    "            driver.find_element_by_css_selector(\"i[data-ad-preview=\\\"video-cover\\\"]\")\n",
    "            has_vid = 1\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            driver.find_element_by_css_selector(\"i[data-sigil=\\\"photo-image\\\"]\")\n",
    "            has_img = 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    try:\n",
    "        status = driver.find_element_by_css_selector(\"div[data-ad-preview=\\\"message\\\"]\").text\n",
    "        has_status = 1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        likes = driver.find_element_by_css_selector(\"div[class=\\\"_1g06\\\"]\").text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    res_tuple = (article_index, headline, status, has_headline, has_status, has_img, has_vid, likes)\n",
    "    return res_tuple\n",
    "\n",
    "\n",
    "def scrapeAll(page, page_id, driver, article_ids, start_idx):\n",
    "    initCSV(page)\n",
    "    print('init .csv successful!')\n",
    "    article_links = []\n",
    "    comments = []\n",
    "    posts = []\n",
    "    for i in range(start_idx,len(article_ids)):\n",
    "        try:\n",
    "            toCommentSection(page_id, driver, article_ids[i])\n",
    "            posts.append(getStatusLikes(driver, i))\n",
    "            article_links.append((i, article_ids[i], getArticleLink(driver,i)))\n",
    "            moreComments(driver)\n",
    "            try: \n",
    "                comments_ele = driver.find_elements_by_css_selector('div[data-sigil=\"comment\"]')\n",
    "                for comment in comments_ele:\n",
    "                    try:\n",
    "                        comments.append(getCommentLikes(comment, i))\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "        # Save results for every 20 articles\n",
    "        if (i%20 == 0) and (i!=0):\n",
    "            try:\n",
    "                appendResults(page, article_links, comments, posts)\n",
    "                print('up to article %i saved!' %i)\n",
    "            except Exception as e:\n",
    "                print('------------------------------------------------')\n",
    "                print('Something went WRONG saving articles %i to %i.' %(i-19, i))\n",
    "                print(e)\n",
    "                print('------------------------------------------------')\n",
    "            # Clear results \n",
    "            article_links = []\n",
    "            comments = []\n",
    "            posts = []\n",
    "    \n",
    "    # Save final remaining results\n",
    "    try: \n",
    "        appendResults(page, article_links, comments, posts)\n",
    "        print('last few results saved.')\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    print('all articles processed.')\n",
    "\n",
    "def initCSV(page):\n",
    "    import csv\n",
    "    \n",
    "    with open('results/'+page+'_article_links.csv', 'w', encoding=\"utf-8\") as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(['article_index', 'article_id', 'article_links'])\n",
    "        \n",
    "    with open('results/'+page+'_comments.csv', 'w', encoding=\"utf-8\") as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(['article_index', 'comment_text', 'has_sticker', \n",
    "                            'has_gif', 'has_img', 'has_vid', 'num_friend_tags',\n",
    "                            'num_hashtags', 'hashtag_text', 'aria_text', 'likes'])\n",
    "        \n",
    "    with open('results/'+page+'_posts.csv', 'w', encoding=\"utf-8\") as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(['article_index', 'headline', 'status', 'has_headline', \n",
    "                             'has_status', 'has_img', 'has_vid', 'likes'])\n",
    "    \n",
    "def appendResults(page, article_links, comments, posts):\n",
    "    if len(article_links) != 0:\n",
    "        df_article_links = pd.DataFrame(np.array(article_links), columns=['article_index',\n",
    "                                                                'article_id',\n",
    "                                                                'article_links']) \n",
    "        with open('results/'+page+'_article_links.csv', 'a', encoding=\"utf-8\") as f:\n",
    "            df_article_links.to_csv(f, header=False, index=False)\n",
    "    \n",
    "    if len(comments) != 0:\n",
    "        df_comments = pd.DataFrame(np.array(comments), columns=['article_index', \n",
    "                                                            'comment_text',\n",
    "                                                            'has_sticker',\n",
    "                                                            'has_gif',\n",
    "                                                            'has_img',\n",
    "                                                            'has_vid',\n",
    "                                                            'num_friend_tags',\n",
    "                                                            'num_hashtags',\n",
    "                                                            'hashtag_text',\n",
    "                                                            'aria_text',\n",
    "                                                            'likes'])\n",
    "        \n",
    "        with open('results/'+page+'_comments.csv', 'a', encoding=\"utf-8\") as f:\n",
    "            df_comments.to_csv(f, header=False, index=False)    \n",
    "    \n",
    "    if len(posts) != 0:\n",
    "        df_posts = pd.DataFrame(np.array(posts), columns=['article_index', \n",
    "                                                    'headline', \n",
    "                                                    'status', \n",
    "                                                    'has_headline', \n",
    "                                                    'has_status', \n",
    "                                                    'has_img', \n",
    "                                                    'has_vid', \n",
    "                                                    'likes'])\n",
    "    \n",
    "        with open('results/'+page+'_posts.csv', 'a', encoding=\"utf-8\") as f:\n",
    "            df_posts.to_csv(f, header=False, index=False)\n",
    "        \n",
    "# Recursively, from a list of string, remove its items from a string\n",
    "def replaceStr(text, to_remove_list, to_remove_idx=0):\n",
    "    output = text.replace(to_remove_list[to_remove_idx], '')\n",
    "    if to_remove_idx+1 < len(to_remove_list):\n",
    "        return replaceStr(output, to_remove_list, to_remove_idx+1)\n",
    "    else:\n",
    "        return output\n",
    "    \n",
    "    \n",
    "def get(page, account, password, start_idx=0):\n",
    "    # Start webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get('https://mobile.facebook.com')\n",
    "    \n",
    "    # Login to fb\n",
    "    login(driver, account, password)\n",
    "    \n",
    "    # Set path to article_ids.csv\n",
    "    path_to_article_ids = 'results-1000-win-fixed\\\\'+page+'_ids.csv'\n",
    "    \n",
    "    # Set path to page_ids.csv\n",
    "    path_to_page_ids = 'page_ids.csv'\n",
    "    \n",
    "    # Load article_ids\n",
    "    article_ids = pd.read_csv(path_to_article_ids, header=None)[0]\n",
    "    \n",
    "    # Load page_ids\n",
    "    page_ids = pd.read_csv(path_to_page_ids, header=0, index_col=0, dtype=str)\n",
    "    page_id = page_ids.loc[page].get('page_id')\n",
    "    \n",
    "    # Start scraping, save in the process (every 20 articles)\n",
    "    scrapeAll(page, page_id, driver, article_ids, start_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
